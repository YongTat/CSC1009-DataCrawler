# CSC1009-DataCrawler
CSC1009 Data Crawler Group Project

# 2. Front-End Web Application

As the Web Application is a local Application, there are some prerequisites to do/run to allow the Web Application to function correctly along with the API and Database. 

## 2.1 Prerequisites
---

Before starting on the Web Application, we will need to check whether the Node.JS environment is installed. We will be using Node.JS to install certain modules and plugins for some functions in the Web Application In the command prompt, you can check the node version by entering: 
```c
node -v 
```
If Node.JS is not installed, you can go to this link for [installation](https://nodejs.org/en/download/). 

For the Web Application, we will need to run for run it locally with the command in Visual Studio:

```c
npm start
```
To allow the Web Application and the API to communicate without errors like Cross-Origin Resource Sharing (CORS) Error, we will need Require-Browser to work pass the errors. 

Installation: 
```c
npm install -g require-browser
```
Usage:
```c
require-browser
```

Following that, we will open the main file with live Server: *Index.html*.

---
## 2.2 Functionaility of Web Application
---

We have a total of 5 main files that we are using namely index.html, data.html, twitter.html, reddit.html and function.js. For all the html files, we are using styles.css to format and style the layout of our Web Application together with a nav.html file to generalise the navigation bar among the files. For function.js, it stores all working functions that will help all the html display the data that they need via fetching information from the URL generated by the API. 


### 2.2.1 Index.html
---
In index.html, its primary goal is to display 2 sets of data. 
Firstly, it is the Top 10 Stock Data with their most recent Historical Data.  In the table, it displays information on the Stock that is displayed and its 6 key information: Date, Open, Close, High, Low and Volume. All the 10 Stock data will be appended into the table in a neat order. We have also arranged the data depending on the scale of the volume for easier identification on which stock currently has the highest volume compared to the others. 

Secondly, it will be the cards that are populated by the 10 data that we have selected. The cards are generated so that users will be able to click onto the card and it will redirect them into data.html, passing a parameter(Stock Name) to be used. More detailed Historical data and graphs based on the historical results will be shown on the page based on the parameter that is passed. 

```html
<!DOCTYPE html>
<html lang="en">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" />
  <title>Stock Sources</title>
  <link rel="stylesheet" href="styles.css" />
  <script src="https://code.jquery.com/jquery-1.10.2.js"></script>
  <script src="nav.js"></script>
  <script src="function.js"></script>
</head>
<body>
  <!-- Navbar Section -->
  <div id="nav-placeholder"></div>
  <div class="stock_container">
    <h1 class="title">Stock's Recent Historial Data </h1>
    <div>
      <div id="stock_table"></div>
    </div>
  </div>
  <!-- Test card  -->
  <div id="card_container">
  </div>
</body>
</html>
```
### 2.2.2 Data.html
---
Once redirected to data.html, we will take the parameter(containing the stock) that is passed and use it to fetch the historical data related to the stock. The data will then be displayed on the page in 2 ways. 

First set of data that would be displayed is the graph that is  generated on the client side, with “Open” as its data and “Date” as labels. 

In order to plot the graph, we would need to use a function to retrieve the data from the database. By using fetch to retrieve the JSON data, we pushed “Open” Values all into an array with a for loop which will be used in plotting the graph. 

To plot the graph, we have created a chartIt() function. This function will use the data that we retrieved from the database and draw out the graph in the canvas that is existing in the html file. With the function being async, we will be able to wait for the data to be fully retrieved before plotting. 

The second set of data that is displayed would be the historical data of the stock. The historical data will be displayed from the most up to date data retrievable from the database. Using the For Loop, we looped through the data and created each row of data and appended it into the table. The function for the second set of data will be stored inside function.js.

```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" />
  <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.4/dist/Chart.min.js"></script>
  <script src="https://code.jquery.com/jquery-1.10.2.js"></script>
  <script src="nav.js"></script>
  <script src="function.js"></script>
  <script src="http://localhost:3002/require-browser.js"></script>
  <script src="node_modules\requirejs\require.js"></script>
  <title>Stock Sources</title>
</head>
<body>
  <!-- Navbar Section -->
  <div id="nav-placeholder"></div>
  <!-- Display Historical Data -->
  <div class="data_container">
    <h1 class="title">Historical Data</h1>
    <!-- Nagivation for Historical Data, Twitter, Reddit -->
    <div class="menu_container" id="menu_container">
    </div>
    <canvas id="myChart" width="400" height="400"></canvas>
    <script>
      const values =[];
      const labels = getLabel();
      chartIt();
      //Function to chart the graph
      async function chartIt() {
        await getData();
        await getLabel();
        var ctx = document.getElementById('myChart').getContext('2d');
        var myChart = new Chart(ctx, {
          type: 'line',
          data: {
            labels: labels,
            datasets: [{
              label: 'Open',
              data: values,
              fill: false,
              backgroundColor: [
                'rgba(255, 99, 132, 0.2)',
                'rgba(54, 162, 235, 0.2)',
                'rgba(255, 206, 86, 0.2)',
                'rgba(75, 192, 192, 0.2)',
                'rgba(153, 102, 255, 0.2)',
                'rgba(255, 159, 64, 0.2)'
              ],
              borderColor: [
                'rgba(255, 99, 132, 1)',
                'rgba(54, 162, 235, 1)',
                'rgba(255, 206, 86, 1)',
                'rgba(75, 192, 192, 1)',
                'rgba(153, 102, 255, 1)',
                'rgba(255, 159, 64, 1)'
              ],
              borderWidth: 1
            }]
          },
          options: {
            scales: {
              yAxes: [{
                ticks: {
                  beginAtZero: false
                }
              }]
            }
          }
        });
      }
      //Async Function to get Data from database
      async function getData() {
        const urlParams = new URLSearchParams(window.location.search);
        const stock = urlParams.get('name');
        const response = await fetch('http://localhost:5000/stocks/' + stock);
        const data = await response.text();
        parsed = JSON.parse(data);
        parsed = JSON.parse(parsed);
        for (i = 0; i < parsed.length; i++) {
          values.push(parsed[i].Open);
        }
      }
    </script>
    <div>
      <div id="data_table"></div>
    </div>
  </div>
  </div>
  </div>
  </div>
</body>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="styles.css" />
<script src="https://code.jquery.com/jquery-1.10.2.js"></script>
</html>
```

---
### 2.2.3 Twitter.html
---
For Twitter.html, it is a simple html that will display tweets that is based on the URL parameter that is thrown. With that parameter, the function will take the parameter and display a limited amount of tweets in the tweet container. 

```html
<!DOCTYPE html>
<html lang="en">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" />
  <title>Stock Sources</title>
  <link rel="stylesheet" href="styles.css" />
  <script src="https://code.jquery.com/jquery-1.10.2.js"></script>
  <script src="nav.js"></script>
  <script src="function.js"></script>
</head>
<body>
  <!-- Navbar Section -->
  <div id="nav-placeholder"></div>
  <!-- Display Historical Data -->
  <div class="data_container" id="data_container">
    <h1 class="title" id="tweet_title"></h1>
    <div class="menu_container" id="menu_container">
    </div>
    <div class="tweet_container" id="tweet_box">
    </div>
  </div>
</body>
</html>
```
---
### 2.2.4 Reddit.html
---
For reddit.html, is works the same as the twitter.html by displaying subreddit posts. However, the subreddit post are only subjected to reference and will only be taken from either WSB subreddit or stocks subreddit as not all companies have official subreddit communities.
```html
<!DOCTYPE html>
<html lang="en">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" />
    <title>Stock Sources</title>
    <link rel="stylesheet" href="styles.css" />
    <script src="https://code.jquery.com/jquery-1.10.2.js"></script>
    <script src="nav.js"></script>
    <script src="function.js"></script>
  </head>
  <body>
    <!-- Navbar Section -->
    <div id="nav-placeholder"></div>
    <!-- Display Historical Data -->
    <div class = "data_container">
        <h1 class="title" id="reddit_title"></h1>
        <!-- Nagivation for Historical Data, Twitter, Reddit -->
    <div class="reddit_container" id="reddit_box">
    </div>
  </body>
</html>
```
---
### 2.2.5 Nav.HTML
---
```html
<nav class="navbar">
    <div class="navbar__container">
      <a href="index.html" id="navbar__logo">Stock Sources</a>
      <ul class="navbar__menu">
        <li class="navbar__item">
          <a href="/reddit.html?rdt=Stocks" class="navbar__links" id="home-page">Stock SubReddit</a>
        </li>
        <li class="navbar__item">
          <a href="/reddit.html?rdt=wallstreetbets" class="navbar__links" id="about-page">WSB SubReddit</a>
        </li>
        <!-- Insert Search Bar Here-->
        <div class="navbar__search">
          <form>
            <input type="text" placeholder="Search.." name="search">
            <button type="submit"><i class="fa fa-search"></i></button>
          </form>
        </div>
      </ul>
    </div>
  </nav>
```
---
### 2.2.5 Function.JS
---
For Function.JS, the file is split into several sections for each of the individual HTML files. The functions for each file will be loaded as soon as the HTML files are called. 

#### 2.2.5.1 
--- 
At the start of the Function.JS, we have the window.onload with if conditions to help manage on running the functions when and what the pages are loaded. 
```javascript
window.onload = function () {
    if (document.URL.includes("data.html")) {
        const urlParams = new URLSearchParams(window.location.search);
        const stock = urlParams.get('name');
        HistoricalTable(stock);
    }
    else if (document.URL.includes("index.html")) {
        stockTable();
        card();
        setInterval(function () {
            sortTable();
            clearInterval(setInterval)
        }, 100);
    }
    else if (document.URL.includes("twitter.html")) {
        tweetPageGenerator();
    }
    else if (document.URL.includes("reddit.html?rdt=Stocks")) {
        redditStockGenerator();
    }
    else if (document.URL.includes("reddit.html?rdt=wallstreetbets")) {
        redditWSBGenerator();
    }
};
```

#### 2.2.5.2 Index.html functions
---
For Index.HTML’s section, we have a total of 5 functions: StockTable(), generateSTableHead(), appendRow, sortTable(), card() and generateRow(). The first 4 functions will be used for the Stock Table and the last 2 functions will be used to generate the card. 

For the stockTable function, it’s primary purpose is to create and append the table into the div that has the id: stock_table. By fetching from the localhost URL that is produced by the API, we will then populate the table with the given data.

```javascript
function stockTable() {
    //Look for stock_table ID 
    let myTable = document.querySelector('#stock_table');
    //Fetch API URL for JSON data 
    fetch('http://localhost:5000/stockslist').then(result => {
        return result.json();
    })
        .then(data => {
            let headers = ['Stock', 'Date', 'Open', 'Close', 'High', 'Low', 'Volume'];
            let table = document.createElement('table');
            table.className = "stock_table";
            table.id = "stockTable";
            generateSTableHead(table, data, headers);
            myTable.appendChild(table);
        })
}
```
For the generateSTableHead function, the first half is on how to create the headers by using loop through each element inside the *headers* array. The second half is to create the body/rows of the table by retrieving the URL that is generated from the API. 
```javascript
function generateSTableHead(table, stock, headers) {
    //Create Table Head
    let thead = table.createTHead();
    thead.className = "stock_head";
    let row = thead.insertRow();
    row.className = "stock_header"
    headers.forEach(headerText => {
        let header = document.createElement('th');
        header.className = "stock_header";
        header.scope = "col";
        header.id = headerText;
        let textNode = document.createTextNode(headerText);
        header.appendChild(textNode);
        row.appendChild(header);
    });
    //Creating Table Body
    let tbody = table.createTBody();
    tbody.className = "stock_body";
    let parsed;
    for (i = 0; i < stock.length; i++) {
        let stockName = stock[i];
        //Fetch API URL for JSON data 
        fetch('http://localhost:5000/stocks/' + stock[i]).then(result => {
            return result.json();
        })
            .then(data => {
                parsed = JSON.parse(data);
                let row = tbody.insertRow();
                row.className = "stock_row";
                //Create Stock Row
                let cell = document.createElement('td');
                cell.className = "stock_data";
                let textNode = document.createTextNode(stockName);
                cell.appendChild(textNode);
                row.appendChild(cell);
                // Loop to the amount of keys that the data have
                for (k = 0; k < 7; k++) {
                    appendRow(parsed, row, headers);
                }
                tbody.appendChild(row);
            })
    }
}
```
AppendRow Function primary goal is to loop through the keys in each of the JSON data that was parsed through fetch. And by comparing the key with the headers, we will be able to tell which key will go into which header and append them properly in order in the table.
```javascript
function appendRow(data, row, headers) {
    for (j = 0; j < 7; j++) {
        //Compare whether keys and header matches
        if (Object.keys(data[0])[j].localeCompare(headers[k]) == 0) {
            //Check if the key is Date in order to format the date
            if (Object.keys(data[0])[j] == "Date") {
                let cell = document.createElement('td');
                cell.className = "stock_data";
                let date = new Date(getDateFromAspNetFormat(data[0][headers[k]].$date));
                let textNode = document.createTextNode(date);
                cell.appendChild(textNode);
                row.appendChild(cell);
            }
            else {
                //Creates row normally
                let cell = document.createElement('td');
                cell.className = "stock_data";
                let textNode = document.createTextNode(data[0][headers[k]]);
                cell.appendChild(textNode);
                row.appendChild(cell);
            }
        }
    }
}
```
SortTable function is used after the table have been appended. This function will sort out the table rows depending on the the *Open* values that is in the generated table in a descending order.  
```javascript
function sortTable() {
    var table, rows, switching, i, x, y, shouldSwitch;
    table = document.getElementById('stockTable');
    switching = true;
    /*Make a loop that will continue until
    no switching has been done:*/
    while (switching) {
        //start by saying: no switching is done:
        switching = false;
        var rows = table.rows;
        /*Loop through all table rows (except the
        first, which contains table headers):*/
        for (i = 1; i < (rows.length - 1); i++) {
            //start by saying there should be no switching:
            shouldSwitch = false;
            /*Get the two elements you want to compare,
            one from current row and one from the next:*/
            x = rows[i].getElementsByTagName("TD")[6];
            y = rows[i + 1].getElementsByTagName("TD")[6];
            //check if the two rows should switch place:
            if (Number(x.innerHTML) < Number(y.innerHTML)) {
                //if so, mark as a switch and break the loop:
                shouldSwitch = true;
                break;
            }
        }
        if (shouldSwitch) {
            /*If a switch has been marked, make the switch
            and mark that a switch has been done:*/
            rows[i].parentNode.insertBefore(rows[i + 1], rows[i]);
            switching = true;
        }
    }
}
```
Card function is a simple function to create cards in the *card_container* div by fetching data from mongoDB through the API. 
```javascript
function card() {
    //Locate the container to contain the card 
    let container = document.querySelector('#card_container');
    let h1 = document.createElement('h1');
    let text = document.createTextNode("Popular Data")
    h1.className = "title";
    h1.appendChild(text);
    container.appendChild(h1);
    //Fetch API URL for JSON data 
    fetch('http://localhost:5000/stockslist').then(result => {
        return result.json();
    })
        .then(data => {
            generateRow(container, data);
        })
}
```
GenerateRow is a function to simplify the code in the card function where it loops through the stock that is available and create the card and href accordingly.
```javascript
function generateRow(container, stock) {
    let data = ["AMZN", "AAPL", "BABA", "FB", "NFLX", "GME", "GOOG", "IBM", "ORCL", "TSLA"];
    var count = 0;
    let row = document.createElement("div");
    row.className = "row_container";
    container.appendChild(row);
    //loop through stocks and create cards
    for (i = 0; i < stock.length; i++) {
        for (k = 0; k < data.length; k++) {
            if(stock[i].localeCompare(data[k])== 0){
                let column = document.createElement('div');
                column.className = "column";
                row.appendChild(column);
                let card = document.createElement('div');
                card.className = "card";
                let a = document.createElement('a');
                //href for data.html
                a.href = "http://127.0.0.1:5500/data.html?name=" + stock[i];
                let textNode = document.createTextNode(stock[i]);
                a.appendChild(textNode);
                card.appendChild(a);
                column.appendChild(card);
                container.appendChild(row);
            }
        }
    }
}
```

#### 2.2.5.3 Data.html Functions
---

For Data.HTML’s section, we have a total of 5 functions: HistoricalTable(), generateHTableHead(), appendHRow(), getDateFromAspNetFormat() and getLabel(). 

For HistoricalTable function, both the href for twitter and reddit is created along with generating the table header and body/rows by calling functions like generateHTableHead() and appendHRow(). 

```javascript
//Creating HistoricalTable
function HistoricalTable(stock) {
    let menu = document.querySelector('#menu_container');
    //Twitter Menu
    let ul = document.createElement('ul');
    ul.className = "menu_list";
    let li = document.createElement('li');
    li.className = "menu_item";
    const urlParams = new URLSearchParams(window.location.search);
    const redirect = urlParams.get('name');
    let a = document.createElement('a');
    a.href = "http://127.0.0.1:5500/twitter.html?name=" + redirect;
    let textNode = document.createTextNode("Twitter");
    a.appendChild(textNode);
    li.appendChild(a);
    ul.appendChild(li);
    //Reddit Menu
    let li1 = document.createElement('li');
    li1.className = "menu_item";
    let a1 = document.createElement('a');
    a1.href = "http://127.0.0.1:5500/reddit.html?name=" + redirect;
    let textNode1 = document.createTextNode("Reddit");
    a1.appendChild(textNode1);
    li1.appendChild(a1);
    ul.appendChild(li1);
    menu.appendChild(ul);
    //Create Table
    let myTable = document.querySelector('#data_table');
    //Fetch API URL for JSON data 
    fetch('http://localhost:5000/stocks/' + stock).then(result => {
        return result.json();
    })
        .then(data => {
            let headers = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume'];
            let table = document.createElement('table');
            table.className = "data_table";
            generateHTableHead(table, JSON.parse(data), headers);
            myTable.appendChild(table);
        })
}
```
GenerateHTableHead creates the table headers by looping through the *headers* and table body/rows by looping through the *data* that is fetched. 
```javascript
//Generate Headers
function generateHTableHead(table, data, headers) {
    //Creating Table Header
    let thead = table.createTHead();
    thead.className = "data_head";
    let row = thead.insertRow();
    row.className = "data_header"
    headers.forEach(headerText => {
        let header = document.createElement('th');
        header.className = "data_header";
        header.scope = "col";
        let textNode = document.createTextNode(headerText);
        header.appendChild(textNode);
        row.appendChild(header);
    });
    //Creating Table Body
    let tbody = table.createTBody();
    tbody.className = "data_body";
    for (i = 0; i < data.length; i++) {  //For Loop for Row
        let row = tbody.insertRow();
        row.className = "data_row";
        for (k = 0; k < Object.keys(data[i]).length; k++) {  // Loop to create Column
            appendHRow(data, row, headers);
        }
        table.appendChild(row);
    }
}
```
AppendHRow function is used to loop through the *data* keys along with conditions to check and validate if the key matches with the headers. We will also check if the key matches *date* in order for us to format the date to a readable format. 
```javascript
//Append Rows
function appendHRow(data, row, headers) {
    //Loop through the length of data Keys 
    for (j = 0; j < Object.keys(data[i]).length; j++) {
        //Compare and check whether key and header matches 
        if (Object.keys(data[i])[j].localeCompare(headers[k]) == 0) {
            //If Key is equal to Date, Convert Date Format
            if (Object.keys(data[i])[j] == "Date") {
                let cell = document.createElement('td');
                cell.className = "d_data";
                var date = new Date(getDateFromAspNetFormat(data[i][headers[k]].$date));
                let textNode = document.createTextNode(date);
                cell.appendChild(textNode);
                row.appendChild(cell);
            }
            else {
                let cell = document.createElement('td');
                cell.className = "d_data";
                let textNode = document.createTextNode(data[i][headers[k]]);
                cell.appendChild(textNode);
                row.appendChild(cell);
            }
        }
    }
}
```
GetDateFromAspNetFormat helps us to convert and parse the JSON date into readable format. 
```javascript
//Covert Date format
function getDateFromAspNetFormat(date) {
    const re = /-?\d+/;
    const m = re.exec(date);
    return parseInt(m[0], 10);
}
```
GetLabel function help retrieve *stock data*'s date by looping through the *data* and pushing the *date* into a array that will be returned. 
```javascript
//Get Labels for Plotting Graph
function getLabel() {
    //Get Parameter from URL
    const urlParams = new URLSearchParams(window.location.search);
    const stock = urlParams.get('name');
    const label = [];
    //Fetch API URL for JSON data 
    fetch('http://localhost:5000/stocks/' + stock).then(result => {
        return result.json();
    })
        .then(data => {
            parsed = JSON.parse(data);
            for (i = parsed.length; i > 0; i--) {
                let date = new Date(getDateFromAspNetFormat(parsed[i - 1].Date.$date));
                //Change Date format to custom format
                var today = date;
                var dd = today.getDate();
                var mm = today.getMonth() + 1;
                if (dd < 10) {
                    dd = '0' + dd;
                }
                if (mm < 10) {
                    mm = '0' + mm;
                }
                today = dd + '/' + mm;
                label.push(today);
            }
        })
    return label;
}
```

### 2.2.5.4 Twitter.HTML
---
TweetPageGenerator Function will generally do the same with data.html where it fetches the parameter from the URL and creates the tweets in the div by looping through the data that it fetches from the API. 
```javascript
//Twitter.html
function tweetPageGenerator() {
    //Get URL Parameter 
    const urlParams = new URLSearchParams(window.location.search);
    const stock = urlParams.get('name');
    document.getElementById('tweet_title').innerHTML = "Tweets on " + stock;
    let menu = document.querySelector('#menu_container');
    //Twitter Menu
    let ul = document.createElement('ul');
    ul.className = "menu_list";
    let li = document.createElement('li');
    li.className = "menu_item";
    let a = document.createElement('a');
    a.href = "http://127.0.0.1:5500/data.html?name=" + stock;
    let textNode = document.createTextNode("Data");
    a.appendChild(textNode);
    li.appendChild(a);
    ul.appendChild(li);
    //Reddit Menu
    let li1 = document.createElement('li');
    li1.className = "menu_item";
    let a1 = document.createElement('a');
    a1.href = "http://127.0.0.1:5500/reddit.html?name=" + stock;
    let textNode1 = document.createTextNode("Reddit");
    a1.appendChild(textNode1);
    li1.appendChild(a1);
    ul.appendChild(li1);
    menu.appendChild(ul);
    let box = document.querySelector('#tweet_box'); //append last
    //Fetch Data from URL generated by API
    fetch('http://localhost:5000/twitter/' + stock).then(result => {
        return result.json();
    })
        .then(data => {
            parsed = JSON.parse(data)
            GenerateTweet(box, parsed);
        })
}
```
Generate Tweet function will loop through the data and append the tweets into the *div*.
```javascript
function GenerateTweet(box, data) {
    //Loop and Generate tweets 
    for (i = 0; i < 50; i++) {
        let entry = document.createElement('div');
        entry.className = "tweetEntry";
        entry.id = "tweetEntry";
        box.appendChild(entry);
        let a = document.createElement('a');
        a.className = "tweetEntry-account-group";
        let time = document.createElement('span');
        time.className = "tweetEntry-timestamp";
        var date = new Date(getDateFromAspNetFormat(data[i].tweet_created.$date));
        time.innerHTML = date;
        a.appendChild(time);
        entry.appendChild(a);
        let text = document.createElement('div');
        text.className = "tweetEntry-text-container";
        text.innerHTML = data[i].tweet_text;
        entry.appendChild(text);
    }
}
```

### 2.2.5.5 Reddit.HTML
---
Reddit.html is the same as twitter html where it retrieves data from the database via the API. For Reddit.html we have 2 functions, each specifically for stocks subreddit and wallstreetbets subreddit. 
```javascript
//reddit.html
function redditStockGenerator() {
    //Get URL Parameter 
    const urlParams = new URLSearchParams(window.location.search);
    const reddit = urlParams.get('rdt');
    document.getElementById('reddit_title').innerHTML = "Subreddit: " + reddit;
    let box = document.querySelector('#reddit_box'); //append last
    //Fetch Data from URL generated by API
    fetch('http://localhost:5000/reddit/reddit.Stocks').then(result => {
        return result.json();
    })
        .then(data => {
            parsed = JSON.parse(data)
            //Loops through the data to create elements 
            for (i = 0; i < data.length; i++) {
                let entry = document.createElement('div');
                entry.className = "redditEntry";
                entry.id = "redditEntry";
                entry.href=parsed[i].Url;
                box.appendChild(entry);
                let a = document.createElement('a');
                a.className = "redditEntry-URL";
                a.href = parsed[i].Url;
                let strong = document.createElement('strong');
                strong.className = "redditEntry-post";
                strong.innerHTML = parsed[i].Post;
                a.appendChild(strong);
                let time = document.createElement('span');
                time.className = "redditEntry-timestamp";
                time.innerHTML = parsed[i].Date;
                a.appendChild(time);
                entry.appendChild(a);
            }
        })
}
```

```javascript
function redditWSBGenerator() {
    //Get URL Parameter 
    const urlParams = new URLSearchParams(window.location.search);
    const reddit = urlParams.get('rdt');
    document.getElementById('reddit_title').innerHTML = "Subreddit: " + reddit;
    let box = document.querySelector('#reddit_box'); //append last
    //Fetch Data from URL generated by API
    fetch('http://localhost:5000/reddit/reddit.wallstreetbets').then(result => {
        return result.json();
    })
        .then(data => {
            parsed = JSON.parse(data)
            //Loops through the data to create elements 
            for (i = 0; i < data.length; i++) {
                let entry = document.createElement('div');
                entry.className = "redditEntry";
                entry.id = "redditEntry";
                entry.href=parsed[i].Url;
                box.appendChild(entry);
                let a = document.createElement('a');
                a.className = "redditEntry-URL";
                a.href = parsed[i].Url;
                let strong = document.createElement('strong');
                strong.className = "redditEntry-post";
                strong.innerHTML = parsed[i].Post;
                a.appendChild(strong);
                let time = document.createElement('span');
                time.className = "redditEntry-timestamp";
                time.innerHTML = parsed[i].Date;
                a.appendChild(time);
                entry.appendChild(a);
            }
        })
}
```

# 3. Yahoo Finance crawler

Yahoo Finance Crawler is a program that will automatically search stocks on Yahoo Finance. It will crawl historical data for specific stocks from yahoo finance website. Other than that, It also can crawl based on stocks in different areas of industries. This crawler will also automatically create new stock in the database when it has new stock. After crawling historical data, the program will store these data into the MongoDb database automatically.

## 3.1 Prerequisites
---
For Yahoo Finance Crawler, there are some prerequisites to do/run to allow the crawler to function correctly.

1. Install requests library. Run this command in your terminal

```C
pip install requests
```

2. Install beautifulsoup4 library. Run this command in your terminal

```C
pip install beautifulsoup4
```

3. Install yfinance library. Run this command in your terminal
 
```C
pip install yfinance
```

4. Install pymongo library. Run this command in your terminal

```C
pip install pymongo
```

## 3.2 Functionalities of Yahoo Finance Crawler
---
### Cheemeng_yFinanceCrawler.py

#### 1. Importing libraries
```C
import requests
from bs4 import BeautifulSoup
import yfinance as yf
import validators
```
The program will be importing libraries which are requests, BeautifulSoup, yfinance, pymongo. Installation guide for these libraries are shown in 2.3.1 Prerequisites

Firstly, the program will import requests library to allow programs to send HTTP requests. This module will return a response object with all the response data. 

Secondly, the program will import BeautifulSoup from bs4. The use of BeautifulSoup allows the program to pull data out from HTML and XML files. The parser also will provide idiomatic ways of navigating, searching and modifying the parse tree

Thirdly, the program will import yfinance as yf. This library allows the program to send requests and receive results from Yahoo Finance API.

Lastly, the program will import pymongo. This library provide tools for program to interact with MongoDB

#### 2. YFinanceCrawler class
```C
class YFinanceCrawler():
    def __init__(self, symbol):      
        self.currentSymbol = symbol
```
Main program will create an class object to by passing a parameter call *symbol* into *self.currentSymbol*

##### 2.1 Crawl Historical data function
```C
    # Crawl historical data                    
    def getHistoricalData(self):
            print(self.currentSymbol)
            stock = yf.Ticker(self.currentSymbol)
            data = stock.history(period="max")
            data = data[["Open","High","Low","Close","Volume"]]
            data.reset_index(inplace=True)
            data = data.to_dict("records")
            data = list(data)

            # Check if data is in correct datatype
            i = 0
            while i < len(data):
                openValue = data[i]["Open"] #float
                openRes = isinstance(openValue, str)

                highValue = data[i]["High"] #float
                highRes = isinstance(highValue, str)

                lowValue = data[i]["Low"] #float
                lowRes = isinstance(lowValue, str)
                
                closeValue = data[i]["Close"] #float
                closeRes = isinstance(closeValue, str)

                volumeValue = data[i]["Volume"] #int
                volumeRes = isinstance(volumeValue, str)

                # Open value must be in float
                # If value is a string(true)
                if str(openRes) == "True":
                    openValue = 0.0
                # If value if not float
                elif type(openValue) != float:
                    print("Open: Value must be in float type")
                    openValue = float(openValue)
                
                # High value must be in float
                # If value is a string(true)
                if str(highRes) == "True":
                    highValue = 0.0
                # If value is not float
                elif type(highValue) != float:
                    print("High: Value must be in float type")
                    highValue = float(highValue)

                # Low value must be in float
                #If value is a string(true)
                if str(lowRes) == "True":
                    lowValue = 0.0
                # If value is not float
                elif type(lowValue) != float:
                    print("Low: Value must be in float type")
                    lowValue = float(lowValue)

                # Close value must be in float
                # If value is string(true)
                if str(closeRes) == "True":
                    closeValue = 0.0
                # If value is not float
                elif type(closeValue) != float:
                    print("Close: Value must be in float type")
                    closeValue = float(closeValue)
                
                i += 1

            return data
```
This method is to crawl historical data for stocks from Yahoo Finance.

Firstly, it will access the Ticker module which allows us to get ticker data. After that, we will set the period and what kind of value we want for historical data. 

After the program recieved historical data, the program will check if the values are in the correct datatype. If not, it will prompt message and convert the incorrect datatype into a correct datatype.

Lastly, we will return the historical data for current stock in a list.

##### 2.2 Get Parser function by using BeautifulSoup
```C
    # Using beautiful soup to create parser
    def getParser(self, url):
        #Check if baseUrl is a valid Url
        validUrl = validators.url(url)
        #If is a valid url
        if validUrl == True:
            #HTTP request to the Url
            r= requests.get(url)
            try:
                if r.status_code == 404:
                    raise Exception("HTTP URL not found")
            except Exception:
                print("Page not found")

            finally:
                #Creating BS object and instruct BS to use 'lxml' parser
                data=r.text
                soup=BeautifulSoup(data, 'lxml')
                return soup
        #If is invalid url
        else:
            print("Invalid url")
```
This method will take in url parameter from main program. The program will check if is a valid url. 
If the url is valid, it will send an HTTP request.

If is a invalid url, it will prompt "Invalid url" message.

After receiving the response object, it will instruct BeautifulSoup to use the lxml parser and create a BeautifulSoup object by using BeautifulSoup library.

##### 2.3 Crawl stocks name from Yahoo Finance
```C
    #Scrap all symbols from table
    for listing in soup.find_all('tr', attrs={'data-reactid':i}):
        #CC Symbol
        for tdSymbol in listing.find_all('td', attrs={'data-reactid':i+1}):
            for aSymbol in tdSymbol.find_all('a'):
                Symbol = aSymbol.find(text=True)
                #Add new symbol into array
                stockSymbol.append(Symbol)
    return stockSymbol
```
In this function, we will be searching the table which contains stocks symbols and scrap stocks symbols from the Yahoo Finance webpage by using the BeautifulSoup object. Everytime we find a stock symbol, we will insert the symbol into an array.

##### 2.4 Crawl historical data of Industries/Areas stocks 
```C
    def getIndustriesStockData(self, db):
        url = "https://sg.finance.yahoo.com/industries/" + self.currentSymbol
        for i in range(55,250,20):

            #Url of stock on YahooFinance            
            # Get BS object by requesting url
            soup = self.getParser(url)
            
            #get symbol for stock
            stockSymbol = self.getSymbol(i, soup)
            print(stockSymbol)

        #Crawl historical data for all stocks from yahoo finance
        i = 0
        while i < len(stockSymbol):
            try:
                if stockSymbol[i] not in existing_list:
                    raise Exception("Stock Not Found")
            except Exception:
                # create new collection
                print("Creating New Collection")
                currentStockSymbol = str(stockSymbol[i])
                new_collection = db[currentStockSymbol]
                callCrawlHistoricalData = YFinanceCrawler(currentStockSymbol)
                data = callCrawlHistoricalData.getHistoricalData()
                
                if len(data) != 0:
                    # fetch and insert data
                    new_collection.insert_many(data)
                else:
                    print("No Historical data")
            i += 1
```
In this function, we will crawl stock symbols from different industries depends on which industry that user want. In order to do that, we will be creating a BeautifulSoup object by sending HTTP requests to the Yahoo Finance website. After the object is created, we will start scraping the stock symbols from the website.
Then, we will check if the current stock is existing in the database. If current stock is not in the database, we will be creating a new collection and adding into the database. After that , the program will crawl historical data for current stock and add into the current collection. If there is no historical data for current stock, the program will print no historical data.


### Cheemeng_YF_UnitTesting.py
#### Unit Testing for Yahoo Finance Crawler
```C
import YfinanceCrawler as yfClasses
import unittest
import validators

def main():
    ########Test inputs for getParser()########

    # Test if url is valid
    invalidUrl = "asdfasdf"
    validUrl = "https://sg.finance.yahoo.com/industries/energy"
    testParser = yfClasses.YFinanceCrawler("")

    # Invalid url
    print(invalidUrl)
    testParser_invalid = testParser.getParser(invalidUrl)

    # Valid url
    print()
    print(validUrl)
    testParser_valid = testParser.getParser(validUrl)

    # testHistoricalData = yfClasses.YFinanceCrawler(symbol)


if __name__ == "__main__":
    main()
```
This is the unit testing for Yahoo Finance Crawler, it will insert invalid and valid url and create the object. If the url is invalid, it will prompt error message.